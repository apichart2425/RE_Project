{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KindScore Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "# from sklearn.decomposition import PCA\n",
    "# from pythainlp.word_vector import thai2vec \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "from pythainlp.tokenize import word_tokenize,sent_tokenize\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "from scipy.cluster import hierarchy\n",
    "import dill as pickle\n",
    "\n",
    "import pythainlp.word_vector\n",
    "# model = word_vector.get_model()\n",
    "# model_path = 'thwiki_data/models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial import distance\n",
    "import numpy.matlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nlp = pythainlp.word_vector.get_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import NLP for Thai\n",
    "<p>GitHub : https://github.com/PyThaiNLP/pythainlp</p>\n",
    "<p>tutorials : https://www.thainlp.org/pythainlp/tutorials/notebooks/pythainlp-get-started.html#Word-Vector</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythainlp import word_tokenize, Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_vectorizer(sentence,dim=300,use_mean=True): # ประกาศฟังก์ชัน sentence_vectorizer\n",
    "#     model_nlp = get_model()\n",
    "\n",
    "    s = word_tokenize(sentence)\n",
    "    vec = np.zeros((1,dim))\n",
    "    for word in s:\n",
    "        if word in model_nlp.index2word: \n",
    "            vec+= model_nlp.word_vec(word)\n",
    "        else: pass\n",
    "    if use_mean: vec /= len(s)\n",
    "    return vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elbow_method(data,k):\n",
    "    clusters = [KMeans(n_clusters = c,init = 'k-means++').fit(data) for c in k]\n",
    "    centr_lst = [cc.cluster_centers_ for cc in clusters]\n",
    "    k_distance = [distance.cdist(data, cent, 'euclidean') for cent in centr_lst]\n",
    "    clust_indx = [np.argmin(kd,axis=1) for kd in k_distance]\n",
    "    distances = [np.min(kd,axis=1) for kd in k_distance]\n",
    "    avg_within = [np.sum(dist)/data.shape[0] for dist in distances]\n",
    "    return avg_within"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elbow_point(curve):\n",
    "    n_points = len(curve)\n",
    "    all_coord = np.vstack((range(n_points), curve)).T\n",
    "    np.array([range(n_points), curve])\n",
    "    first_point = all_coord[0]\n",
    "    line_vec = all_coord[-1] - first_point\n",
    "    line_vec_norm = line_vec / np.sqrt(np.sum(line_vec**2))\n",
    "    vec_from_first = all_coord - first_point\n",
    "    scalar_product = np.sum(vec_from_first * np.matlib.repmat(line_vec_norm, n_points, 1), axis=1)\n",
    "    vec_from_first_parallel = np.outer(scalar_product, line_vec_norm)\n",
    "    vec_to_line = vec_from_first - vec_from_first_parallel\n",
    "    dist_to_line = np.sqrt(np.sum(vec_to_line ** 2, axis=1))\n",
    "#     print(dist_to_line)\n",
    "    idx_best = np.argmax(dist_to_line)\n",
    "    return idx_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KindScoreModel(dataset, featureOne, featureTwo):\n",
    "    df = dataset\n",
    "    text = df[featureOne].dropna()\n",
    "    sentences_newmm= []\n",
    "    \n",
    "    for x in text:\n",
    "        proc_newmm = word_tokenize(x, engine='newmm', keep_whitespace=False)\n",
    "        lst_newmm = []\n",
    "\n",
    "        for word in proc_newmm:\n",
    "            if(word != \" \"):\n",
    "                lst_newmm.append(word)\n",
    "        sentences_newmm.append(lst_newmm)\n",
    "        \n",
    "    lst_vector = []\n",
    "    for i in text:\n",
    "        lst_vector.append(sentence_vectorizer(i)[0])\n",
    "    X=np.array(lst_vector)\n",
    "\n",
    "    wcss = []\n",
    "    for i in range(1,11):\n",
    "        kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 42)\n",
    "        kmeans.fit(X)\n",
    "        wcss.append(kmeans.inertia_)\n",
    "\n",
    "    elbow=elbow_point(wcss)\n",
    "\n",
    "    n = len(wcss)+1\n",
    "    K= range(1, n)\n",
    "\n",
    "    print(\"Optimal Cluster Number: \",K[elbow])\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.plot(K, wcss, 'bx-')\n",
    "    ax.plot(K[elbow], wcss[elbow], marker='o', markersize=12,markeredgewidth=2, markeredgecolor='r', markerfacecolor='None')\n",
    "\n",
    "    plt.grid()\n",
    "    plt.xlabel('Number of clusters')\n",
    "    plt.ylabel('Average within-cluster sum of squares')\n",
    "    plt.title('Elow for KMeans clustering')\n",
    "    plt.show();\n",
    "\n",
    "    n_clusters = K[elbow]\n",
    "    clf = KMeans(n_clusters=n_clusters,max_iter=100,init='k-means++',n_init=1)\n",
    "    labels = clf.fit_predict(X)\n",
    "    labels\n",
    "    \n",
    "    cluster = []\n",
    "    for index, sentence in enumerate(sentences_newmm):\n",
    "        cluster.append(labels[index])\n",
    "\n",
    "    text_cluster = pd.DataFrame({\"cluster\":cluster})  \n",
    "    \n",
    "    df_cluster = df [[featureOne,featureTwo]]\n",
    "    df_cluster[\"cluster\"] = text_cluster.values\n",
    "    df_cluster\n",
    "    return df_cluster,clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_similarity(s1,s2):\n",
    "    for i in s2:\n",
    "        cosine_similarity(sentence_vectorizer(str(s1)),sentence_vectorizer(str(i)))\n",
    "    return cosine_similarity(sentence_vectorizer(str(s1)),sentence_vectorizer(str(s2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Weightscore(path : str, sentence, dataset,model, pathNotmacth: str):\n",
    "    text = sentence\n",
    "    pathFile = path+'.csv'\n",
    "    pathNotmacth = pathNotmacth+\".csv\"\n",
    "    cluster = []\n",
    "    l = []\n",
    "    Newproblem = []\n",
    "    Oldproblem = []\n",
    "    similarity = []\n",
    "    \n",
    "    Textproblem = [] #text[countgroup],\n",
    "    Clusterproblem = [] #newProblem[countgroup]\n",
    "    Scoreproblem = [] #state_score\n",
    "\n",
    "\n",
    "    for word in text:\n",
    "        l.append(sentence_vectorizer(word)[0])\n",
    "    KL=np.array(l)\n",
    "    \n",
    "    newProblem = model.predict(KL)\n",
    "    cluster.append(newProblem)\n",
    "    \n",
    "    for countgroup in range(len(newProblem)):\n",
    "        state_score = \"\"\n",
    "        state_similarity = 0\n",
    "        oldtext = \"\"\n",
    "        for index in dataset[dataset['cluster'] == newProblem[countgroup]].values:\n",
    "\n",
    "            check_similarity = sentence_similarity(text[countgroup], index[0])\n",
    "            if(check_similarity > state_similarity):\n",
    "                state_similarity = check_similarity\n",
    "                state_score = index[1]\n",
    "                oldtext =  index[0]\n",
    "        print(text[countgroup],state_score,state_similarity[0][0],oldtext)\n",
    "        if(state_similarity >= 0.7):\n",
    "            print(\"state_similarity >= 0.7 || 1\")\n",
    "            Textproblem.append(text[countgroup])\n",
    "            Scoreproblem.append(state_score)\n",
    "            Clusterproblem.append(newProblem[countgroup])\n",
    "            if text[countgroup] not in dataset.Problem.tolist():\n",
    "                dataset.loc[dataset.shape[0]]= [text[countgroup],state_score,newProblem[countgroup]];\n",
    "        else:\n",
    "            print(\"state_similarity <= 0.7\")\n",
    "            for index in dataset[dataset['cluster'] != newProblem[countgroup]].values:\n",
    "                check_similarity = sentence_similarity(text[countgroup], index[0])\n",
    "                if(check_similarity > state_similarity):\n",
    "                    state_similarity = check_similarity\n",
    "                    state_score = index[1]\n",
    "                    oldtext =  index[0]\n",
    "\n",
    "            if(state_similarity >= 0.7):\n",
    "                print(\"state_similarity >= 0.7 || 2\")\n",
    "                Textproblem.append(text[countgroup])\n",
    "                Scoreproblem.append(state_score)\n",
    "                Clusterproblem.append(newProblem[countgroup])\n",
    "                if text[countgroup] not in dataset.Problem.tolist():\n",
    "                    dataset.loc[dataset.shape[0]]= [text[countgroup],state_score,newProblem[countgroup]];\n",
    "            else:\n",
    "                \n",
    "                Newproblem.append(text[countgroup])\n",
    "                Oldproblem.append(oldtext)\n",
    "                similarity.append(state_similarity[0][0])\n",
    "        \n",
    "    with open(pathFile, 'w',encoding='utf-8-sig') as f:\n",
    "        dataset.to_csv(f, header=True,index=False)\n",
    "\n",
    "    with open(pathNotmacth, 'w',encoding='utf-8-sig') as f:\n",
    "        data_notmatch = pd.DataFrame({\"Newproblem\":Newproblem,\"Oldproblem\":Oldproblem,\"similarity\":similarity})\n",
    "        data_notmatch.to_csv(f, header=True,index=False)\n",
    "        \n",
    "    return dataset, pd.DataFrame({\"Problem\":Textproblem,\"score\":Scoreproblem,\"cluster\":Clusterproblem})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "def pieplot(df):\n",
    "    \n",
    "    def func(pct, allvals):\n",
    "        absolute = int(pct/100.*np.sum(allvals))\n",
    "        return \"{:.1f}%\\n({:d})\".format(pct, absolute)\n",
    "\n",
    "    sizes = df.groupby(\"cluster\").count()['Problem']\n",
    "    labels = [\"cluster \"+str(I) for I in df.groupby(\"cluster\").count()['Problem'].index[:]]\n",
    "    # Plot\n",
    "    plt.pie(sizes,colors=sns.color_palette(\"pastel\"), labels=labels,autopct=lambda pct: func(pct, sizes), shadow=True, startangle=140, textprops=dict(color=\"black\"))\n",
    "    plt.legend(\n",
    "              title=\"Cluster\",\n",
    "              loc=\"center left\",\n",
    "              bbox_to_anchor=(1, 0, 0.5, 1))\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.axis('equal')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
