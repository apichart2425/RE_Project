{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pythainlp import word_tokenize\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "# from pythainlp.word_vector import thai2vec \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "# %matplotlib inline\n",
    "\n",
    "from pythainlp.tokenize import word_tokenize\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "from scipy.cluster import hierarchy\n",
    "import dill as pickle\n",
    "import pandas as pd\n",
    "import pythainlp.word_vector\n",
    "# model = word_vector.get_model()\n",
    "model_path = 'thwiki_data/models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thai2dict = {}\n",
    "# for word in model.index2word:\n",
    "#     thai2dict[word] = model[word]\n",
    "# thai2vec = pd.DataFrame.from_dict(thai2dict,orient='index')\n",
    "# thai2vec.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = [\"ลองตัดคำดูก่อน\", \"จะได้ไหมนะ\",\"ทดสอบการตัด\" , \"ตัดคำในภาษาไทย\", \"ตัดได้จริงไหม\", \"ตัดคำได้ด้วยอะ\", \"วันนี้กินข้าวอะไร\", \"กินก๋วยเตี๋ยว\", \"กินก๋วยเตี๋ยว\", \"กินก๋วยเตี๋ยว\" ,\"ไปกินอาหารกันไหม\", \"ไม่อยากกินแล้วอะ\"]\n",
    "# text = [\"วันนี้กินกะเพรา\", \"วันนี้กินก๋วยเตี๋ยว\", \"วันนี้ไปกินน้ำส้มกัน\", \"เล่นเกมปะ\"\n",
    "#        ,\"ไม่ชอบกินก๋วยเตี๋ยวเลย\", \"ผมชอบไปกินข้าวผัด\", \"ชอบเล่นเกม\",\"ชอบเล่นฮีโรในแอล\",\n",
    "#         ]\n",
    "# text = [\"this is the learning good deep good book\", \"this is another book\", \"one more book\", \"train railway station\", \"time train station\", \"time railway station train\", \"this is the new post\", \"this is about more deep learning post\", \"and this is the one\"]\n",
    "text = [\"ผมเป็นนักศึกษามหาวิทยาลัยขอนแก่น\",\"วันนี้กินข้าวอะไร\", \"กินก๋วยเตี๋ยว\", \"ชอบเล่นเกม\",\"ลองตัดคำดูก่อน\", \"จะได้ไหมนะ\",\"ทดสอบการตัด\" , \"ตัดคำในภาษาไทย\", \"ตัดได้จริงไหม\",\"ไปกินอาหารกันไหม\", \"ไม่อยากกินแล้วอะ\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['ผม', 'เป็น', 'นักศึกษา', 'มหาวิทยาลัยขอนแก่น'], ['วันนี้', 'กินข้าว', 'อะไร'], ['กิน', 'ก๋วยเตี๋ยว'], ['ชอบ', 'เล่น', 'เกม'], ['ลอง', 'ตัด', 'คำ', 'ดูก่อน']]\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "# for y in text:\n",
    "#     proc = y.split()\n",
    "#     sentences.append(proc)\n",
    "# print(sentences)\n",
    "for x in text:\n",
    "    proc = word_tokenize(x, engine='newmm')\n",
    "#     textafter.append(proc)\n",
    "    sentences.append(proc)\n",
    "print(sentences)\n",
    "model = pythainlp.word_vector.get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.01613275 -0.117553    0.32987126 ...  0.129998   -0.25389801\n",
      "   0.09044775]\n",
      " [ 0.04093099 -0.08288367  0.20193534 ...  0.07557733 -0.054721\n",
      "  -0.08182567]\n",
      " [-0.0402395   0.072976   -0.0113545  ...  0.34258401  0.2355705\n",
      "   0.0142415 ]\n",
      " [-0.03928367  0.02922533  0.04883    ...  0.08057033 -0.098258\n",
      "  -0.14573533]\n",
      " [ 0.0127515  -0.00460674  0.0574335  ...  0.06342725 -0.156816\n",
      "   0.17575775]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "def sentence_vectorizer(ss,dim=300,use_mean=True): # ประกาศฟังก์ชัน sentence_vectorizer\n",
    "    s = word_tokenize(ss)\n",
    "    vec = np.zeros((1,dim))\n",
    "    for word in s:\n",
    "        if word in model.wv.index2word:\n",
    "            vec+= model.wv.word_vec(word)\n",
    "        else: pass\n",
    "    if use_mean: vec /= len(s)\n",
    "    return vec\n",
    "\n",
    "l = []\n",
    "c = 0\n",
    "for i in text:\n",
    "    l.append(sentence_vectorizer(i)[0])\n",
    "#     if(c==0):\n",
    "#         print(sentence_vectorizer(i)[0])\n",
    "#     c+=1\n",
    "X=np.array(l)\n",
    "print(X)\n",
    "\n",
    "# m = Word2Vec(sentences, size=50, min_count=1, sg=1)\n",
    "# def vectorizer(sent,m):\n",
    "#     vec = []\n",
    "#     numw = 0\n",
    "#     for w in sent:\n",
    "#         try:\n",
    "#             if numw == 0:\n",
    "#                 vec = m[w]\n",
    "#             else:\n",
    "#                 vec = np.add(vec, m[w])\n",
    "#             numw+=1\n",
    "#         except:\n",
    "#             pass\n",
    "#         return np.asarray(vec) / numw\n",
    "# l = []\n",
    "# c=0\n",
    "# for i in sentences:\n",
    "#     l.append(vectorizer(i, m))\n",
    "# #     if(c==0):\n",
    "# #         print(vectorizer(i,m))\n",
    "# #     c+=1\n",
    "# X=np.array(l)\n",
    "# print(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.23502779]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sentence_similarity(s1,s2):\n",
    "    return cosine_similarity(sentence_vectorizer(str(s1)),sentence_vectorizer(str(s2)))\n",
    "sentence_similarity(\"วันนี้กินกระเพรา\", \"ไปเชียงใหม่\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "n_samples=5 should be >= n_clusters=6",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-afc45c04be52>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mkmeans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_clusters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'k-means++'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mkmeans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mwcss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkmeans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minertia_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwcss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/cluster/k_means_.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    970\u001b[0m                 \u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy_x\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    971\u001b[0m                 \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malgorithm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malgorithm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 972\u001b[0;31m                 return_n_iter=True)\n\u001b[0m\u001b[1;32m    973\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    974\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/cluster/k_means_.py\u001b[0m in \u001b[0;36mk_means\u001b[0;34m(X, n_clusters, sample_weight, init, precompute_distances, n_init, max_iter, verbose, tol, random_state, copy_x, n_jobs, algorithm, return_n_iter)\u001b[0m\n\u001b[1;32m    314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mn_clusters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m         raise ValueError(\"n_samples=%d should be >= n_clusters=%d\" % (\n\u001b[0;32m--> 316\u001b[0;31m             _num_samples(X), n_clusters))\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0mtol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tolerance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: n_samples=5 should be >= n_clusters=6"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "wcss = []\n",
    "for i in range(1,10):\n",
    "    kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 42)\n",
    "    kmeans.fit(X)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "plt.plot(range(1,10), wcss)\n",
    "plt.title('Elbow')\n",
    "plt.xlabel('Num')\n",
    "plt.ylabel('WCSS')\n",
    "plt.show()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 3\n",
    "clf = KMeans(n_clusters=n_clusters,\n",
    "            max_iter=100,\n",
    "            init='k-means++',\n",
    "            n_init=1)\n",
    "labels = clf.fit_predict(X)\n",
    "print(labels)\n",
    "for index, sentence in enumerate(sentences):\n",
    "    print(str(labels[index]) + \":\" + str(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca  = PCA(n_components=2).fit(X)\n",
    "coords = pca.transform(X)\n",
    "label_colors = [\"#54FF9F\", \"#7D26CD\", \"#794044\", \"#F4E628\"]\n",
    "colors = [label_colors[i] for i in labels]\n",
    "plt.scatter(coords[:, 0], coords[:, 1], c=colors)\n",
    "centroids = clf.cluster_centers_\n",
    "centroid_coords = pca.transform(centroids)\n",
    "plt.scatter(centroid_coords[:, 0], centroid_coords[:, 1], marker='X', s=200, linewidths=2, c='#444d61')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = sentence_vectorizer(\"งงจังครับ\")\n",
    "l = []\n",
    "c = 0\n",
    "# for i in text:\n",
    "l.append(sentence_vectorizer(\"ชอบเล่นเกม\")[0])\n",
    "#     if(c==0):\n",
    "#         print(sentence_vectorizer(i)[0])\n",
    "#     c+=1\n",
    "KL=np.array(l)\n",
    "clf.predict(KL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = hierarchy.linkage(X, 'ward')\n",
    "dn = hierarchy.dendrogram(Z)\n",
    "plt.title('Dendogram')\n",
    "plt.ylabel('Euclodean')\n",
    "plt.xlim(0,100)\n",
    "plt.ylim(0,0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hc = AgglomerativeClustering(n_clusters=3,\n",
    "                             affinity='euclidean',\n",
    "                            linkage='ward')\n",
    "y_hc= hc.fit_predict(X)\n",
    "print(y_hc)\n",
    "for index, sentence in enumerate(sentences):\n",
    "    print(str(y_hc[index]) + \":\" + str(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
