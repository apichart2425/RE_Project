{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Model Sklearn & NLP to fit value text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pythainlp import word_tokenize\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "# from sklearn.decomposition import PCA\n",
    "# from pythainlp.word_vector import thai2vec \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythainlp.tokenize import word_tokenize,sent_tokenize\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "from scipy.cluster import hierarchy\n",
    "import dill as pickle\n",
    "import pandas as pd\n",
    "import pythainlp.word_vector\n",
    "# model = word_vector.get_model()\n",
    "model_path = 'thwiki_data/models/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read date problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/dataReV5.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df['Problem'].dropna().rename(columns = {\"Problem\": \"problem\"})\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import NLP Spit text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythainlp import word_tokenize, Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_newmm= []\n",
    "sentences_longest= []\n",
    "\n",
    "for x in text:\n",
    "    proc_newmm = word_tokenize(x, engine='newmm', keep_whitespace=False)\n",
    "    proc_longest = word_tokenize(x, engine='longest', keep_whitespace=False)\n",
    "\n",
    "    lst_newmm = []\n",
    "    lst_longest = []\n",
    "    \n",
    "    for word in proc_newmm:\n",
    "        if(word != \" \"):\n",
    "            lst_newmm.append(word)\n",
    "    sentences_newmm.append(lst_newmm)\n",
    "    for word in proc_longest:\n",
    "        if(word != \" \"):\n",
    "            lst_longest.append(word)\n",
    "    sentences_longest.append(lst_longest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nlp = pythainlp.word_vector.get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_vectorizer(ss,dim=300,use_mean=True): # ประกาศฟังก์ชัน sentence_vectorizer\n",
    "    s = word_tokenize(ss)\n",
    "    vec = np.zeros((1,dim))\n",
    "    for word in s:\n",
    "#         if word in model_nlp.wv.index2word: \n",
    "#             vec+= model_nlp.wv.word_vec(word)\n",
    "        if word in model_nlp.index2word: \n",
    "            vec+= model_nlp.word_vec(word)\n",
    "        else: pass\n",
    "    if use_mean: vec /= len(s)\n",
    "    return vec\n",
    "\n",
    "lst_vector = []\n",
    "for i in text:\n",
    "    lst_vector.append(sentence_vectorizer(i)[0])\n",
    "X=np.array(lst_vector)\n",
    "print(X)\n",
    "# text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "wcss = []\n",
    "for i in range(1,11):\n",
    "    kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 42)\n",
    "    kmeans.fit(X)\n",
    "    wcss.append(kmeans.inertia_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Import  scipy.spatial  distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "import numpy.matlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elbow_method(data,k):\n",
    "    clusters = [KMeans(n_clusters = c,init = 'k-means++').fit(data) for c in k]\n",
    "    centr_lst = [cc.cluster_centers_ for cc in clusters]\n",
    "    k_distance = [distance.cdist(data, cent, 'euclidean') for cent in centr_lst]\n",
    "    clust_indx = [np.argmin(kd,axis=1) for kd in k_distance]\n",
    "    distances = [np.min(kd,axis=1) for kd in k_distance]\n",
    "    avg_within = [np.sum(dist)/data.shape[0] for dist in distances]\n",
    "    return avg_within"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elbow_point(curve):\n",
    "    n_points = len(curve)\n",
    "    all_coord = np.vstack((range(n_points), curve)).T\n",
    "    np.array([range(n_points), curve])\n",
    "    first_point = all_coord[0]\n",
    "    line_vec = all_coord[-1] - first_point\n",
    "    line_vec_norm = line_vec / np.sqrt(np.sum(line_vec**2))\n",
    "    vec_from_first = all_coord - first_point\n",
    "    scalar_product = np.sum(vec_from_first * np.matlib.repmat(line_vec_norm, n_points, 1), axis=1)\n",
    "    vec_from_first_parallel = np.outer(scalar_product, line_vec_norm)\n",
    "    vec_to_line = vec_from_first - vec_from_first_parallel\n",
    "    dist_to_line = np.sqrt(np.sum(vec_to_line ** 2, axis=1))\n",
    "    print(dist_to_line)\n",
    "    idx_best = np.argmax(dist_to_line)\n",
    "    return idx_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elbow=elbow_point(wcss)\n",
    "elbow.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(wcss)+1\n",
    "K= range(1, n)\n",
    "\n",
    "print(\"Optimal Cluster Number: \",K[elbow])\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(K, wcss, 'bx-')\n",
    "ax.plot(K[elbow], wcss[elbow], marker='o', markersize=12,markeredgewidth=2, markeredgecolor='r', markerfacecolor='None')\n",
    "# ax.set_xticks(np.arange(0,n,1))\n",
    "# ax.set_yticks(np.arange(0,max(wcss),0.5))\n",
    "plt.grid()\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Average within-cluster sum of squares')\n",
    "plt.title('Elow for KMeans clustering')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = K[elbow]\n",
    "clf = KMeans(n_clusters=n_clusters,\n",
    "            max_iter=100,\n",
    "            init='k-means++',\n",
    "            n_init=1)\n",
    "labels = clf.fit_predict(X)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_clusters = K[elbow]\n",
    "clf = KMeans(n_clusters=n_clusters,\n",
    "            max_iter=100,\n",
    "            init='k-means++',\n",
    "            n_init=1)\n",
    "labels = clf.fit_predict(X)\n",
    "labels\n",
    "cluster = []\n",
    "value = []\n",
    "o = \"\"\n",
    "for index, sentence in enumerate(sentences_newmm):\n",
    "#     print(str(labels[index]) + \":\" + str(sentence))\n",
    "    cluster.append(labels[index])\n",
    "#     value.append(o.join(sentences_newmm[index]))\n",
    "#     list_of_cluster = list(zip(cluster))\n",
    "text_cluster = pd.DataFrame({\"cluster\":cluster})  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df_cluster = pd.DataFrame(list_of_cluster, columns = ['cluster', 'text'])  \n",
    "df_cluster = df [['Problem','score']]\n",
    "df_cluster.append(text_cluster, ignore_index = True)\n",
    "df_cluster[\"cluster\"] = text_cluster.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_cluster = pd.DataFrame(list_of_cluster, columns = ['cluster', 'text'])  \n",
    "df_cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Speech Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import speech_recognition as sr\n",
    "# Record Audio\n",
    "r = sr.Recognizer()\n",
    "m = sr.Microphone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set threhold level\n",
    "with m as source: r.adjust_for_ambient_noise(source)\n",
    "print(\"Set minimum energy threshold to {}\".format(r.energy_threshold))\n",
    "\n",
    "# Speech recognition using Google Speech Recognition\n",
    "def checkspeech(r):\n",
    "#     file_wav\n",
    "#     sr.Microphone()  sr.WavFile(file_wav)\n",
    "#     with sr.WavFile(\"C:/Users/Pack.Apichart/Desktop/re/t1.wav\")\n",
    "    print(\"Wake UP!!\")\n",
    "    with  sr.Microphone()  as source:\n",
    "        audio = r.listen(source)\n",
    "    try:\n",
    "    # for testing purposes, we're just using the default API key\n",
    "    # to use another API key, use `r.recognize_google(audio, key=\"GOOGLE_SPEECH_RECOGNITION_API_KEY\")`\n",
    "    # instead of `r.recognize_google(audio)`\n",
    "\n",
    "# \"en-US\"\n",
    "# \"th-TH\"\n",
    "        print(\"You said: \" + r.recognize_google(audio,language = \"th-TH\"))\n",
    "        return r.recognize_google(audio,language = \"th-TH\")\n",
    "#         return (r.recognize_google(audio))\n",
    "    except sr.UnknownValueError:\n",
    "        print(\"Google Speech Recognition could not understand audio\")\n",
    "        return \"stop now\"\n",
    "    except sr.RequestError as e:\n",
    "        print(\"Could not request results from Google Speech Recognition service; {0}\".format(e))\n",
    "        return \"Could not request results from Google Speech Recognition service\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# speech = str(checkspeech(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# speech_list = []\n",
    "# speech_list.append(sentence_vectorizer(str(speech))[0])\n",
    "# speech_vector = np.array(speech_list)\n",
    "# result = clf.predict(speech_vector)\n",
    "# result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weigth score Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# speech = str(checkspeech(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# speech_list = []\n",
    "# speech_list.append(sentence_vectorizer(str(speech))[0])\n",
    "# speech_vector = np.array(speech_list)\n",
    "# speech_result = clf.predict(speech_vector)\n",
    "# speech_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weigth score Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sentence_similarity(s1,s2):\n",
    "#     for i in s2:\n",
    "#         print(cosine_similarity(sentence_vectorizer(str(s1)),sentence_vectorizer(str(i))))\n",
    "# #     return cosine_similarity(sentence_vectorizer(str(s1)),sentence_vectorizer(str(s2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function weigth score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_similarity(s1,s2):\n",
    "    for i in s2:\n",
    "        cosine_similarity(sentence_vectorizer(str(s1)),sentence_vectorizer(str(i)))\n",
    "    return cosine_similarity(sentence_vectorizer(str(s1)),sentence_vectorizer(str(s2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(sentence_similarity(\"มีเวลาน้อย\",\"มีเวลาพักไม่พอต่อคิว\"))\n",
    "# print(sentence_similarity(\"มีเวลาน้อย\",\"มีเวลาน้อยไม่สามารถต่อคิวได้\"))\n",
    "# print(sentence_similarity(\"มีเวลาน้อย\",\"มีเวลาพักกินข้าวน้อย\"))\n",
    "# print(sentence_similarity(\"มีเวลาน้อย\",\"ร้านอาหารมีจำนวนไม่มากพอที่จะรับจำนวนนักศึกษา\"))\n",
    "# print(sentence_similarity(\"มีเวลาน้อย\",\"ร้านอาหารมีคิวเยอะ\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_cluster[df_cluster['cluster']==speech_result[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "text2222 = [\"รอคิวนาน\", \"เบื่อเมนูอาหาร\", \"คิวเยอะ\", \"อาหารหมด\"]\n",
    "group_text = []\n",
    "l = []\n",
    "\n",
    "for y in text2222:\n",
    "    l.append(sentence_vectorizer(y)[0])\n",
    "KL=np.array(l)\n",
    "labels_newProblem = clf.predict(KL)\n",
    "group_text.append(labels_newProblem)\n",
    "\n",
    "for countgroup in range(len(labels_newProblem)):\n",
    "    state_score = \"\"\n",
    "    state_similarity = 0\n",
    "    for i in df_cluster[df_cluster['cluster']==labels_newProblem[countgroup]].values:\n",
    "        check_similarity = sentence_similarity(text2222[countgroup],i[0])\n",
    "        if(check_similarity > state_similarity):\n",
    "            state_similarity = check_similarity\n",
    "            state_score = i[1]\n",
    "\n",
    "#         print(check_similarity,i[0],\": score\",i[1])\n",
    "    print(text2222[countgroup], \"group: \", labels_newProblem[countgroup], \"similarity: \" ,state_similarity[0][0], \"score :\" , state_score)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vehicle\trestaurant\tfood delivery\tProblem\tscore\twork\tเวลาในการรออาหาร\tราคาอาหาร"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_cluster[df_cluster['cluster']==labels_newProblem[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sentence_similarity(\"มีเวลาน้อย\",\"มีเวลาพักกินข้าวน้อย\"))\n",
    "l = []\n",
    "\n",
    "# l.append(sentence_vectorizer(\"ถ้าร้านนั้นคนเยอะก็จะไปเลือกร้านอื่น\")[0])\n",
    "l.append(sentence_vectorizer(\"มีเวลาน้อย\")[0])\n",
    "\n",
    "KL=np.array(l)\n",
    "KL\n",
    "labels = clf.predict(KL)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WRITE SCORE TO DATA SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  new_df = pd.DataFrame({\n",
    "#         'Vehicle' : [''],\n",
    "#         'restaurant' : [''],\n",
    "#         'food delivery' : [''],\n",
    "#         'Problem' : [labels_newProblem[countgroup]],\n",
    "#         'score' : [state_score],\n",
    "#         'work' : [''],\n",
    "#         'เวลาในการรออาหาร' : [''],\n",
    "#         'ราคาอาหาร' : ['']\n",
    "#     })\n",
    "#     new_df.to_csv('data/dataReV5.csv', mode='a', header=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
