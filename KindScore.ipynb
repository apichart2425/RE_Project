{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KindScore Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "# from sklearn.decomposition import PCA\n",
    "# from pythainlp.word_vector import thai2vec \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "from pythainlp.tokenize import word_tokenize,sent_tokenize\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "from scipy.cluster import hierarchy\n",
    "import dill as pickle\n",
    "\n",
    "import pythainlp.word_vector\n",
    "# model = word_vector.get_model()\n",
    "# model_path = 'thwiki_data/models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial import distance\n",
    "import numpy.matlib\n",
    "# import speech_recognition as sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nlp = pythainlp.word_vector.get_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import NLP for Thai\n",
    "<p>GitHub : https://github.com/PyThaiNLP/pythainlp</p>\n",
    "<p>tutorials : https://www.thainlp.org/pythainlp/tutorials/notebooks/pythainlp-get-started.html#Word-Vector</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythainlp import word_tokenize, Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.models.keyedvectors import Word2VecKeyedVectors\n",
    "# from gensim.models import KeyedVectors\n",
    "# from pythainlp.corpus import get_corpus, get_corpus_path\n",
    "\n",
    "# def _download() -> str:\n",
    "#     path = get_corpus_path(\"thai2fit_wv\")\n",
    "#     if not path:\n",
    "#         download_data(\"thai2fit_wv\")\n",
    "#         path = get_corpus_path(\"thai2fit_wv\")\n",
    "#     return path\n",
    "\n",
    "# def get_model() -> Word2VecKeyedVectors:\n",
    "#     \"\"\"\n",
    "#     Download model\n",
    "#     :return: `gensim` word2vec model\n",
    "#     :rtype: gensim.models.keyedvectors.Word2VecKeyedVectors\n",
    "#     \"\"\"\n",
    "#     return KeyedVectors.load_word2vec_format(_download(), binary=True)\n",
    "\n",
    "\n",
    "# _MODEL = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_vectorizer(sentence,dim=300,use_mean=True): # ประกาศฟังก์ชัน sentence_vectorizer\n",
    "#     model_nlp = get_model()\n",
    "\n",
    "    s = word_tokenize(sentence)\n",
    "    vec = np.zeros((1,dim))\n",
    "    for word in s:\n",
    "        if word in model_nlp.index2word: \n",
    "            vec+= model_nlp.word_vec(word)\n",
    "        else: pass\n",
    "    if use_mean: vec /= len(s)\n",
    "    return vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elbow_method(data,k):\n",
    "    clusters = [KMeans(n_clusters = c,init = 'k-means++').fit(data) for c in k]\n",
    "    centr_lst = [cc.cluster_centers_ for cc in clusters]\n",
    "    k_distance = [distance.cdist(data, cent, 'euclidean') for cent in centr_lst]\n",
    "    clust_indx = [np.argmin(kd,axis=1) for kd in k_distance]\n",
    "    distances = [np.min(kd,axis=1) for kd in k_distance]\n",
    "    avg_within = [np.sum(dist)/data.shape[0] for dist in distances]\n",
    "    return avg_within"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elbow_point(curve):\n",
    "    n_points = len(curve)\n",
    "    all_coord = np.vstack((range(n_points), curve)).T\n",
    "    np.array([range(n_points), curve])\n",
    "    first_point = all_coord[0]\n",
    "    line_vec = all_coord[-1] - first_point\n",
    "    line_vec_norm = line_vec / np.sqrt(np.sum(line_vec**2))\n",
    "    vec_from_first = all_coord - first_point\n",
    "    scalar_product = np.sum(vec_from_first * np.matlib.repmat(line_vec_norm, n_points, 1), axis=1)\n",
    "    vec_from_first_parallel = np.outer(scalar_product, line_vec_norm)\n",
    "    vec_to_line = vec_from_first - vec_from_first_parallel\n",
    "    dist_to_line = np.sqrt(np.sum(vec_to_line ** 2, axis=1))\n",
    "#     print(dist_to_line)\n",
    "    idx_best = np.argmax(dist_to_line)\n",
    "    return idx_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KindScoreModel(dataset, featureOne, featureTwo):\n",
    "    df = dataset\n",
    "    text = df[featureOne].dropna()\n",
    "    sentences_newmm= []\n",
    "    \n",
    "    for x in text:\n",
    "        proc_newmm = word_tokenize(x, engine='newmm', keep_whitespace=False)\n",
    "        lst_newmm = []\n",
    "\n",
    "        for word in proc_newmm:\n",
    "            if(word != \" \"):\n",
    "                lst_newmm.append(word)\n",
    "        sentences_newmm.append(lst_newmm)\n",
    "        \n",
    "    lst_vector = []\n",
    "    for i in text:\n",
    "        lst_vector.append(sentence_vectorizer(i)[0])\n",
    "    X=np.array(lst_vector)\n",
    "\n",
    "    wcss = []\n",
    "    for i in range(1,11):\n",
    "        kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 42)\n",
    "        kmeans.fit(X)\n",
    "        wcss.append(kmeans.inertia_)\n",
    "\n",
    "    elbow=elbow_point(wcss)\n",
    "\n",
    "    n = len(wcss)+1\n",
    "    K= range(1, n)\n",
    "\n",
    "    print(\"Optimal Cluster Number: \",K[elbow])\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.plot(K, wcss, 'bx-')\n",
    "    ax.plot(K[elbow], wcss[elbow], marker='o', markersize=12,markeredgewidth=2, markeredgecolor='r', markerfacecolor='None')\n",
    "    # ax.set_xticks(np.arange(0,n,1))\n",
    "    # ax.set_yticks(np.arange(0,max(wcss),0.5))\n",
    "    plt.grid()\n",
    "    plt.xlabel('Number of clusters')\n",
    "    plt.ylabel('Average within-cluster sum of squares')\n",
    "    plt.title('Elow for KMeans clustering')\n",
    "    plt.show();\n",
    "\n",
    "    n_clusters = K[elbow]\n",
    "    clf = KMeans(n_clusters=n_clusters,max_iter=100,init='k-means++',n_init=1)\n",
    "    labels = clf.fit_predict(X)\n",
    "    labels\n",
    "    \n",
    "    cluster = []\n",
    "    for index, sentence in enumerate(sentences_newmm):\n",
    "        cluster.append(labels[index])\n",
    "\n",
    "    text_cluster = pd.DataFrame({\"cluster\":cluster})  \n",
    "    \n",
    "    df_cluster = df [[featureOne,featureTwo]]\n",
    "    df_cluster[\"cluster\"] = text_cluster.values\n",
    "    df_cluster\n",
    "    return df_cluster,clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set minimum energy threshold to 48.62553160386514\n"
     ]
    }
   ],
   "source": [
    "# # Record Audio\n",
    "# r = sr.Recognizer()\n",
    "# m = sr.Microphone()\n",
    "# #set threhold level\n",
    "# with m as source: r.adjust_for_ambient_noise(source)\n",
    "# print(\"Set minimum energy threshold to {}\".format(r.energy_threshold))\n",
    "\n",
    "# # Speech recognition using Google Speech Recognition\n",
    "# def SpeechRecognition():\n",
    "# #     file_wav\n",
    "# #     sr.Microphone()  sr.WavFile(file_wav)\n",
    "# #     with sr.WavFile(\"C:/Users/Pack.Apichart/Desktop/re/t1.wav\")\n",
    "#     print(\"Wake UP!! : said something\")\n",
    "#     with  sr.Microphone()  as source:\n",
    "#         audio = r.listen(source)\n",
    "#     try:\n",
    "#     # for testing purposes, we're just using the default API key\n",
    "#     # to use another API key, use `r.recognize_google(audio, key=\"GOOGLE_SPEECH_RECOGNITION_API_KEY\")`\n",
    "#     # instead of `r.recognize_google(audio)`\n",
    "\n",
    "# # \"en-US\"\n",
    "# # \"th-TH\"\n",
    "#         print(\"You said: \" + r.recognize_google(audio,language = \"th-TH\"))\n",
    "#         return r.recognize_google(audio,language = \"th-TH\")\n",
    "#     except sr.UnknownValueError:\n",
    "#         print(\"Google Speech Recognition could not understand audio\")\n",
    "#         return \"stop now\"\n",
    "#     except sr.RequestError as e:\n",
    "#         print(\"Could not request results from Google Speech Recognition service; {0}\".format(e))\n",
    "#         return \"Could not request results from Google Speech Recognition service\"\n",
    "\n",
    "# def FileRecognition(file_wav,duration):\n",
    "#     print(\"Processing !!\")\n",
    "#     with sr.WavFile(file_wav) as source:\n",
    "#         audio = r.listen(source,duration)\n",
    "#     try:\n",
    "#     # for testing purposes, we're just using the default API key\n",
    "#     # to use another API key, use `r.recognize_google(audio, key=\"GOOGLE_SPEECH_RECOGNITION_API_KEY\")`\n",
    "#     # instead of `r.recognize_google(audio)`\n",
    "\n",
    "# # \"en-US\"\n",
    "# # \"th-TH\"\n",
    "#         print(\"You said: \" + r.recognize_google(audio,language = \"th-TH\"))\n",
    "#         return r.recognize_google(audio,language = \"th-TH\")\n",
    "# #         return (r.recognize_google(audio))\n",
    "#     except sr.UnknownValueError:\n",
    "#         print(\"Google Speech Recognition could not understand audio\")\n",
    "#         return \"stop now\"\n",
    "#     except sr.RequestError as e:\n",
    "#         print(\"Could not request results from Google Speech Recognition service; {0}\".format(e))\n",
    "#         return \"Could not request results from Google Speech Recognition service\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_similarity(s1,s2):\n",
    "    for i in s2:\n",
    "        cosine_similarity(sentence_vectorizer(str(s1)),sentence_vectorizer(str(i)))\n",
    "    return cosine_similarity(sentence_vectorizer(str(s1)),sentence_vectorizer(str(s2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Weightscore(path, sentence, dataset, model):\n",
    "    text = sentence\n",
    "    cluster = []\n",
    "    l = []\n",
    "    \n",
    "    for word in text:\n",
    "        l.append(sentence_vectorizer(word)[0])\n",
    "    KL=np.array(l)\n",
    "    \n",
    "    newProblem = model.predict(KL)\n",
    "    cluster.append(newProblem)\n",
    "    \n",
    "    for countgroup in range(len(newProblem)):\n",
    "        state_score = \"\"\n",
    "        state_similarity = 0\n",
    "        for index in dataset[dataset['cluster'] == newProblem[countgroup]].values:\n",
    "            check_similarity = sentence_similarity(text[countgroup], index[0])\n",
    "            if(check_similarity > state_similarity):\n",
    "                state_similarity = check_similarity\n",
    "                state_score = index[1]\n",
    "\n",
    "        new_df = pd.DataFrame(\n",
    "                    {\n",
    "                'Vehicle' : [''],\n",
    "                'restaurant' : [''],\n",
    "                'food delivery' : [''],\n",
    "                'Problem' : [text[countgroup]],\n",
    "                'score' : [state_score],\n",
    "                'work' : [''],\n",
    "                'เวลาในการรออาหาร' : [''],\n",
    "                'ราคาอาหาร' : ['']\n",
    "            })\n",
    "        print(text[countgroup], \"group: \", newProblem[countgroup], \"similarity: \" ,state_similarity[0][0], \"score :\" , state_score)\n",
    "\n",
    "        new_df.to_csv(path+\".csv\", mode='a', header=False, index=False)\n",
    "    \n",
    "\n",
    "    return pd.read_csv(path+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
